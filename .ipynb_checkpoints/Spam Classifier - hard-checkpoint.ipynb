{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f991d2ab",
   "metadata": {},
   "source": [
    "# Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b000095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "HARD_HAM_URL = DOWNLOAD_ROOT + \"20021010_hard_ham.tar.bz2\"\n",
    "HAM_2_URL = DOWNLOAD_ROOT + \"20030228_easy_ham_2.tar.bz2\"\n",
    "SPAM_2_URL = DOWNLOAD_ROOT + \"20030228_spam_2.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"full_spam\")\n",
    "\n",
    "URLS = (\n",
    "    (\"ham.tar.bz2\", HAM_URL, \"ham\"), \n",
    "    (\"spam.tar.bz2\", SPAM_URL, \"spam\"),\n",
    "    (\"spam2.tar.bz2\", SPAM_2_URL, \"spam\"),\n",
    "    (\"hard_ham.tar.bz2\", HARD_HAM_URL, \"ham\"),\n",
    "    (\"ham2.tar.bz2\", HAM_2_URL, \"ham\")\n",
    ")\n",
    "\n",
    "def fetch_spam_data(urls=URLS, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "\n",
    "    for filename, url, category in urls:\n",
    "        category_path = os.path.join(spam_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            os.makedirs(category_path) \n",
    "\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path) \n",
    "\n",
    "        with tarfile.open(path) as tar_bz2_file:\n",
    "            for member in tar_bz2_file.getmembers():\n",
    "                # Remove subdiretórios e extrai apenas arquivos\n",
    "                if member.isfile():  \n",
    "                    member.name = os.path.basename(member.name)  # Remove caminhos extras\n",
    "                    tar_bz2_file.extract(member, path=category_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df889fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5729ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11794932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148d5794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1897"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33fee2",
   "metadata": {},
   "source": [
    "# Parsing the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b97a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        # Retorna um objeto EmailMessage\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a9199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7d8146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Return-Path',\n",
       " 'Delivered-To',\n",
       " 'Received',\n",
       " 'Received',\n",
       " 'Received',\n",
       " 'Received',\n",
       " 'Delivered-To',\n",
       " 'Received',\n",
       " 'Received',\n",
       " 'Received',\n",
       " 'Received',\n",
       " 'Received',\n",
       " 'Received',\n",
       " 'From',\n",
       " 'To',\n",
       " 'Cc',\n",
       " 'Subject',\n",
       " 'In-Reply-To',\n",
       " 'References',\n",
       " 'MIME-Version',\n",
       " 'Content-Type',\n",
       " 'Message-Id',\n",
       " 'X-Loop',\n",
       " 'Sender',\n",
       " 'Errors-To',\n",
       " 'X-Beenthere',\n",
       " 'X-Mailman-Version',\n",
       " 'Precedence',\n",
       " 'List-Help',\n",
       " 'List-Post',\n",
       " 'List-Subscribe',\n",
       " 'List-Id',\n",
       " 'List-Unsubscribe',\n",
       " 'List-Archive',\n",
       " 'Date']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_emails[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a846563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Wednesday 11 September 2002 16:19 CET Justin Mason wrote:\n",
      "> Malte S. Stretz said:\n",
      ">[...]\n",
      "> > I think we should even add new (GA'd) rules to 2.4x (and/or remove old\n",
      "> > ones) and tag a new 2.50 only if we have a bunch of features worth a\n",
      "> > \"dangerous\" big update. I'd say: Yes, you should expect 2.42 and also\n",
      "> > 2.43+ (but update to 2.41 now).\n",
      ">\n",
      "> I would think adding new rules to, or removing broken rules from, 2.4x\n",
      "> would require some discussion first.  but new GA'd scores are definitely\n",
      "> worth putting in, as the ones there are too wild.\n",
      "\n",
      "I think my mail wasn't very clear ;-) My point was that we should continue \n",
      "releasing new rules and removing broken ones (all based on discussions on \n",
      "this list of course) in the 2.4 branch instead of creating a new 2.5 branch \n",
      "everytime we have a bunch of new rules.\n",
      "\n",
      "A new branch should be openend only if (big) new features are introduced \n",
      "(eg. Bayes) or the interface has changed (spam_level_char=x). As the rules \n",
      "are under fluent development, the user has to update quite regularly. But \n",
      "currently he couldn't be shure if the new release will break anything in \n",
      "his setup (like -F going away). So if we say \"the branches are stable to \n",
      "the outside and just improved under the surface but you have to watch out \n",
      "when you update to a new minor version number\", users and sysadmins could \n",
      "be less reluctant to update.\n",
      "\n",
      "All just IMHO :o)\n",
      "Malte\n",
      "\n",
      "P.S.: I'll be away from my box and my mail account for one week, starting \n",
      "tomorrow. So happy coding for the next week :-)\n",
      "\n",
      "-- \n",
      "--- Coding is art.\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[3000].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93426e05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML>\n",
      "<BODY>\n",
      "</head>\n",
      "\n",
      "<body bgcolor=\"#FFFFFF\">\n",
      "<table width=\"62%\" height=\"218\">\n",
      "  <tr valign=\"top\"> \n",
      "    <td height=\"260\"> \n",
      "      <p><font size=\"3\"><b>Fortunes are literally being made in this great new \n",
      "        marketplace!</b></font></p>\n",
      "      <p>O<font size=\"3\"></font><font size=\"3\">ver <b>$9 Billion</b> in merchandise \n",
      "        was sold on <b>eBay</b> in 2001 by people just like you - <u>right from \n",
      "        their homes</u>! </font></p>\n",
      "      <p><font size=\"3\">Now you too can learn the secrets of <b>successful selling \n",
      "        </b>on<b> eBay</b> and <b>make a staggering income</b> from the comfort \n",
      "        of <b>your own home</b>. If you are <b>motivated</b>, capable of having \n",
      "        an<b> open mind</b>, and can follow simple directions, then <a href=\"http://www.generaledu.com/ebayepubs\">visit \n",
      "        us here.</a> If server busy - <a href=\"http://168.75.161.164/ebayepubs/\">alternate.</a></font></p>\n",
      "      <p><font size=\"2\"> <font size=\"1\">We are strongly against sending unsolicited \n",
      "        emails to those who do not wish to receive our special mailings. You have \n",
      "        opted in to one or more of our affiliate sites requesting to be notified \n",
      "        of any special offers we may run from time to time. We also have attained \n",
      "        the services of an independent 3rd party to overlook list management and \n",
      "        removal services. This is NOT unsolicited email. If you do not wish to \n",
      "        receive further mailings, please <a href=\"http://www.generaledu.com/remove.html\">GO \n",
      "        HERE</a> to be removed from the list. Please accept our apologies if you \n",
      "        have been sent this email in error. </font></font></p>\n",
      "      </td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p>&nbsp;</p>\n",
      "<p>&nbsp; </p>\n",
      "<p>&nbsp;</p>\n",
      "<p>&nbsp; </p>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "<p><p><p><p><p><p><p><p><p><p>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<html><p><head><p><meta http-equiv=\"Content-Type\" content=\"text/html; <p>charset=iso-8859-1\"><p><p><p><p><p><p><p><p><p>\n",
      "</BODY>\n",
      "</HTML>\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[900].get_content().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f663c3",
   "metadata": {},
   "source": [
    "## Multipart Emails\n",
    "\n",
    "Some emails are multipart, which means they have different sections with different kinds of content (plain text, images, HTML...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a628d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        # Se for multipart, retorna uma lista de partes\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "264dae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Armazena a frequencia de cada tipo de email em um dicionario\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42dfd1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 3837),\n",
       " ('text/html', 122),\n",
       " ('multipart(text/plain, application/pgp-signature)', 101),\n",
       " ('multipart(text/plain, text/html)', 58),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 2),\n",
       " ('multipart(text/plain, multipart(text/plain))', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  2),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 2),\n",
       " ('multipart(text/html)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/x-patch)', 1),\n",
       " ('multipart(multipart(text/plain, multipart(text/plain), text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(multipart(text/plain, text/html), image/jpeg, image/gif, image/gif, image/gif, image/gif)',\n",
       "  1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, application/ms-tnef)', 1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, text/plain, text/plain)', 1),\n",
       " ('multipart(text/plain, image/png, image/png)', 1),\n",
       " ('multipart(multipart(text/plain, text/html))', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1),\n",
       " ('multipart(text/plain, image/bmp)', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72dee10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 816),\n",
       " ('text/html', 772),\n",
       " ('multipart(text/plain, text/html)', 159),\n",
       " ('multipart(text/html)', 49),\n",
       " ('multipart(text/plain)', 44),\n",
       " ('multipart(multipart(text/html))', 23),\n",
       " ('multipart(multipart(text/plain, text/html))', 5),\n",
       " ('multipart(text/plain, application/octet-stream, text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 3),\n",
       " ('multipart(text/html, text/plain)', 3),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart/alternative', 2),\n",
       " ('multipart(text/html, image/jpeg)', 2),\n",
       " ('multipart(multipart(text/plain), application/octet-stream)', 2),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg)',\n",
       "  1),\n",
       " ('multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/gif)',\n",
       "  1),\n",
       " ('text/plain charset=us-ascii', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart(multipart(text/html), image/gif)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream, application/octet-stream, application/octet-stream)',\n",
       "  1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif, image/jpeg)', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1169075",
   "metadata": {},
   "source": [
    "É possível notar que parte majoritária de emails que não são spams são constituidos unicamente de texto, enquanto que emails de spam possuem bastante estruturas HTML. Além disso a estrutura \"application/octet-stream\" (usado quando o servidor ou cliente de e-mail não consegue determianr exatamente o tipo do arquivo) é mais comum em spams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730a0b9",
   "metadata": {},
   "source": [
    "## Email Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c71573a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <ilug-admin@linux.ie>\n",
      "Delivered-To : yyyy@localhost.netnoteinc.com\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 9E1F5441DD\tfor <jm@localhost>; Tue,  6 Aug 2002 06:48:09 -0400 (EDT)\n",
      "Received : from phobos [127.0.0.1]\tby localhost with IMAP (fetchmail-5.9.0)\tfor jm@localhost (single-drop); Tue, 06 Aug 2002 11:48:09 +0100 (IST)\n",
      "Received : from lugh.tuatha.org (root@lugh.tuatha.org [194.125.145.45]) by    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g72LqWv13294 for    <jm-ilug@jmason.org>; Fri, 2 Aug 2002 22:52:32 +0100\n",
      "Received : from lugh (root@localhost [127.0.0.1]) by lugh.tuatha.org    (8.9.3/8.9.3) with ESMTP id WAA31224; Fri, 2 Aug 2002 22:50:17 +0100\n",
      "Received : from bettyjagessar.com (w142.z064000057.nyc-ny.dsl.cnc.net    [64.0.57.142]) by lugh.tuatha.org (8.9.3/8.9.3) with ESMTP id WAA31201 for    <ilug@linux.ie>; Fri, 2 Aug 2002 22:50:11 +0100\n",
      "X-Authentication-Warning : lugh.tuatha.org: Host w142.z064000057.nyc-ny.dsl.cnc.net    [64.0.57.142] claimed to be bettyjagessar.com\n",
      "Received : from 64.0.57.142 [202.63.165.34] by bettyjagessar.com    (SMTPD32-7.06 EVAL) id A42A7FC01F2; Fri, 02 Aug 2002 02:18:18 -0400\n",
      "Message-Id : <1028311679.886@0.57.142>\n",
      "Date : Fri, 02 Aug 2002 23:37:59 +0530\n",
      "To : ilug@linux.ie\n",
      "From : Start Now <startnow2002@hotmail.com>\n",
      "MIME-Version : 1.0\n",
      "Content-Type : text/plain; charset=\"US-ASCII\"; format=\"flowed\"\n",
      "Subject : [ILUG] STOP THE MLM INSANITY\n",
      "Sender : ilug-admin@linux.ie\n",
      "Errors-To : ilug-admin@linux.ie\n",
      "X-Mailman-Version : 1.1\n",
      "Precedence : bulk\n",
      "List-Id : Irish Linux Users' Group <ilug.linux.ie>\n",
      "X-Beenthere : ilug@linux.ie\n"
     ]
    }
   ],
   "source": [
    "for header, value in spam_emails[0].items():\n",
    "    print(header, \":\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de5f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <exmh-workers-admin@spamassassin.taint.org>\n",
      "Delivered-To : yyyy@localhost.netnoteinc.com\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 7106643C34\tfor <jm@localhost>; Wed, 21 Aug 2002 08:33:03 -0400 (EDT)\n",
      "Received : from phobos [127.0.0.1]\tby localhost with IMAP (fetchmail-5.9.0)\tfor jm@localhost (single-drop); Wed, 21 Aug 2002 13:33:03 +0100 (IST)\n",
      "Received : from listman.spamassassin.taint.org (listman.spamassassin.taint.org [66.187.233.211]) by    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g7LCXvZ24654 for    <jm-exmh@jmason.org>; Wed, 21 Aug 2002 13:33:57 +0100\n",
      "Received : from listman.spamassassin.taint.org (localhost.localdomain [127.0.0.1]) by    listman.redhat.com (Postfix) with ESMTP id F12A13EA25; Wed, 21 Aug 2002    08:34:00 -0400 (EDT)\n",
      "Delivered-To : exmh-workers@listman.spamassassin.taint.org\n",
      "Received : from int-mx1.corp.spamassassin.taint.org (int-mx1.corp.spamassassin.taint.org    [172.16.52.254]) by listman.redhat.com (Postfix) with ESMTP id 750D33F945    for <exmh-workers@listman.redhat.com>; Wed, 21 Aug 2002 08:30:55 -0400    (EDT)\n",
      "Received : (from mail@localhost) by int-mx1.corp.spamassassin.taint.org (8.11.6/8.11.6)    id g7LCUqx17585 for exmh-workers@listman.redhat.com; Wed, 21 Aug 2002    08:30:52 -0400\n",
      "Received : from mx1.spamassassin.taint.org (mx1.spamassassin.taint.org [172.16.48.31]) by    int-mx1.corp.redhat.com (8.11.6/8.11.6) with SMTP id g7LCUqY17578 for    <exmh-workers@redhat.com>; Wed, 21 Aug 2002 08:30:52 -0400\n",
      "Received : from ratree.psu.ac.th ([202.28.97.6]) by mx1.spamassassin.taint.org    (8.11.6/8.11.6) with SMTP id g7LCGNl23207 for <exmh-workers@redhat.com>;    Wed, 21 Aug 2002 08:16:24 -0400\n",
      "Received : from delta.cs.mu.OZ.AU (delta.coe.psu.ac.th [172.30.0.98]) by    ratree.psu.ac.th (8.11.6/8.11.6) with ESMTP id g7LCUIl27286;    Wed, 21 Aug 2002 19:30:19 +0700 (ICT)\n",
      "Received : from munnari.OZ.AU (localhost [127.0.0.1]) by delta.cs.mu.OZ.AU    (8.11.6/8.11.6) with ESMTP id g7LCU1W09629; Wed, 21 Aug 2002 19:30:01    +0700 (ICT)\n",
      "From : Robert Elz <kre@munnari.OZ.AU>\n",
      "To : Chris Garrigues <cwg-dated-1030314468.7c7c85@DeepEddy.Com>\n",
      "Cc : exmh-workers@spamassassin.taint.org\n",
      "Subject : Re: New Sequences Window\n",
      "In-Reply-To : <1029882468.3116.TMDA@deepeddy.vircio.com>\n",
      "References : <1029882468.3116.TMDA@deepeddy.vircio.com>\n",
      "MIME-Version : 1.0\n",
      "Content-Type : text/plain; charset=\"us-ascii\"\n",
      "Message-Id : <9627.1029933001@munnari.OZ.AU>\n",
      "X-Loop : exmh-workers@spamassassin.taint.org\n",
      "Sender : exmh-workers-admin@spamassassin.taint.org\n",
      "Errors-To : exmh-workers-admin@spamassassin.taint.org\n",
      "X-Beenthere : exmh-workers@spamassassin.taint.org\n",
      "X-Mailman-Version : 2.0.1\n",
      "Precedence : bulk\n",
      "List-Help : <mailto:exmh-workers-request@spamassassin.taint.org?subject=help>\n",
      "List-Post : <mailto:exmh-workers@spamassassin.taint.org>\n",
      "List-Subscribe : <https://listman.spamassassin.taint.org/mailman/listinfo/exmh-workers>,    <mailto:exmh-workers-request@redhat.com?subject=subscribe>\n",
      "List-Id : Discussion list for EXMH developers <exmh-workers.spamassassin.taint.org>\n",
      "List-Unsubscribe : <https://listman.spamassassin.taint.org/mailman/listinfo/exmh-workers>,    <mailto:exmh-workers-request@redhat.com?subject=unsubscribe>\n",
      "List-Archive : <https://listman.spamassassin.taint.org/mailman/private/exmh-workers/>\n",
      "Date : Wed, 21 Aug 2002 19:30:01 +0700\n"
     ]
    }
   ],
   "source": [
    "for header, value in ham_emails[0].items():\n",
    "    print(header, \":\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff20d223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ILUG] STOP THE MLM INSANITY'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0][\"Subject\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0fdd8",
   "metadata": {},
   "source": [
    "## Spliting training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b34b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0]* len(ham_emails) + [1]*len(spam_emails))\n",
    "\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e856ec3",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ef08c",
   "metadata": {},
   "source": [
    "First, we need to convert HTML to plain text. Here we can use html2text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3563edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<body>\n",
      "<p align=\"center\"><br>\n",
      "<b><font size=\"6\" face=\"Tahoma\" color=\"#000080\">\n",
      "Norton SystemWorks 2002<br>Software Suite<br>\n",
      "Professional Edition</font><br><br>\n",
      "</b><font face=\"Arial\"><b><font size=\"5\"><font color=\"#FF0000\">\n",
      "6 Feature-Packed Utilities,</font> 1 Great Price</font><br>\n",
      "<font size=\"4\">A $300.00+ Combined Retail Value for <font color=\"#FF0000\"> Only $29.99!</font></font><br><br>\n",
      "<font size=\"3\"><span style=\"background-color: #FFFF00\">\n",
      "Protect your computer and your valuable information!<br><br>\n",
      "Don't allow yourself to fall prey to destructive viruses!</span><br><br>\n",
      "<a href=\"http://www.1800mailman.com/software/sw.htm\">CLICK HERE FOR MORE INFO AND TO ORDER</a></font></b><br><br>\n",
      "<font size=\"2\">If you wish to unsubscribe from this list, please <a href=\"http://www.1800mailman.com/removeme.html\"> Click Here</a> to be removed.</font></font></p>\n",
      "</body>\n",
      "</html> ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                   if get_email_structure(email) == \"text/html\"]\n",
    "                    \n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d484ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "**Norton SystemWorks 2002  \n",
      "Software Suite  \n",
      "Professional Edition  \n",
      "  \n",
      "****6 Feature-Packed Utilities, 1 Great Price  \n",
      "A $300.00+ Combined Retail Value for  Only $29.99!  \n",
      "  \n",
      "Protect your computer and your valuable information!  \n",
      "  \n",
      "Don't allow yourself to fall prey to destructive viruses!  \n",
      "  \n",
      "[CLICK HERE FOR MORE INFO AND TO\n",
      "ORDER](http://www.1800mailman.com/software/sw.htm)**  \n",
      "  \n",
      "If you wish to unsubscribe from this list, please [ Click\n",
      "Here](http://www.1800mailman.com/removeme.html) to be removed.\n",
      "\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "from html2text import html2text\n",
    "print(html2text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6165b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except:\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html2text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe80e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "**Norton SystemWorks 2002  \n",
      "Software Suite  \n",
      "Professional Edition  \n",
      "  \n",
      "****6 Feature-Packed Utili\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4ba3c",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6453dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "    print(word, \"=>\", stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5647c04",
   "metadata": {},
   "source": [
    "We also need to replace URLs with the word \"URL\", for this we will use urlextract library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa6e0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
     ]
    }
   ],
   "source": [
    "import urlextract # may require an Internet connection to download root domain names\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3927e39",
   "metadata": {},
   "source": [
    "## Word Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d1d03",
   "metadata": {},
   "source": [
    "Now we can put all this together intro a transformer that we will use to convert emails to word counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c25b1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "\n",
    "# Transformer personalizado\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    \n",
    "    # Fit não faz nada, pois esse transformer não aprende com os dados\n",
    "    # usado para manter compatibilidade com scikit learn\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # Converte os emails em contagem de palavras\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        \n",
    "        for email in X:\n",
    "            \n",
    "            # Email para texto\n",
    "            text = email_to_text(email) or \"\"\n",
    "            \n",
    "            # Maiusculas para minusculas\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "                \n",
    "            # Substituir URLs\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            \n",
    "            # Substituir numeros\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            \n",
    "            # Substituir pontuação\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            \n",
    "            # conta palavras\n",
    "            word_counts = Counter(text.split())\n",
    "            \n",
    "            # Reduz as palavras para raiz\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e1f6f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'number': 9, 'and': 6, 'the': 5, 'signal': 4, 'to': 4, 'our': 4, 'advanc': 3, 'cell': 3, 'phone': 3, 'free': 3, 'of': 3, 'each': 3, 'your': 3, 'click': 3, 'world': 2, 'most': 2, 'booster': 2, 'hand': 2, 'headset': 2, 'super': 2, 'is': 2, 'technolog': 2, 'antenna': 2, 'now': 2, 'from': 2, 'purchas': 2, 'in': 2, 'less': 2, 'call': 2, 'ani': 2, 'or': 2, 'on': 2, 'at': 2, 'if': 2, 'you': 2, 'web': 2, 's': 1, 'intern': 1, 'adapt': 1, 'all': 1, 'make': 1, 'model': 1, 'mobil': 1, 'normal': 1, 'price': 1, 'base': 1, 'upon': 1, 'unit': 1, 'bonu': 1, 'radiat': 1, 'shield': 1, 'with': 1, 'util': 1, 'same': 1, 'aerospac': 1, 'use': 1, 'by': 1, 'state': 1, 'art': 1, 'satellit': 1, 'dish': 1, 'receiv': 1, 'which': 1, 'result': 1, 'drop': 1, 'dead': 1, 'zone': 1, 'improv': 1, 'recept': 1, 'build': 1, 'elev': 1, 'hallway': 1, 'tunnel': 1, 'mountain': 1, 'place': 1, 'where': 1, 'a': 1, 'weak': 1, 'don': 1, 't': 1, 'miss': 1, 'that': 1, 'urgent': 1, 'love': 1, 'one': 1, 'dure': 1, 'an': 1, 'emerg': 1, 'natur': 1, 'disast': 1, 'critic': 1, 'situat': 1, 'get': 1, 'also': 1, 'check': 1, 'out': 1, 'lightweight': 1, 'cellular': 1, 'below': 1, 'websit': 1, 'for': 1, 'more': 1, 'inform': 1, 'url': 1, 'are': 1, 'have': 1, 'problem': 1, 'link': 1, 'abov': 1, 'then': 1, 'copi': 1, 'past': 1, 'address': 1, 'into': 1, 'browser': 1, 'wish': 1, 'no': 1, 'longer': 1, 'be': 1, 'includ': 1, 'mail': 1, 'list': 1, 'pleas': 1, 'here': 1, 'mailto': 1, 'remov': 1, 'noucenumbercom': 1, 'write': 1, 'us': 1, 'noucenumb': 1, 'numbernd': 1, 'ave': 1, 'n': 1, 'st': 1, 'petersburg': 1, 'fl': 1}),\n",
       "       Counter({'number': 11, 'to': 11, 'i': 10, 'a': 8, 'rpm': 8, 'the': 7, 'on': 6, 'for': 6, 'and': 6, 'test': 6, 'that': 6, 'my': 5, 'apt': 5, 'it': 5, 'use': 5, 'redhat': 5, 'one': 4, 'own': 4, 'srpm': 4, 's': 4, 'mirror': 4, 'wa': 4, 'at': 3, 'have': 3, 'rh': 3, 'funet': 3, 'be': 3, 'thi': 3, 'base': 3, 'is': 3, 'as': 3, 'of': 3, 'list': 3, 'feb': 2, 'numberpm': 2, 'wrote': 2, 'main': 2, 't': 2, 'repositori': 2, 'directori': 2, 'like': 2, 'currentnumb': 2, 'current': 2, 'gccnumber': 2, 'os': 2, 'network': 2, 'updat': 2, 'stuff': 2, 'topdir': 2, 'someon': 2, 'put': 2, 'url': 2, 'machin': 2, 'veri': 2, 'think': 2, 'but': 2, 'also': 2, 'project': 2, 'seem': 2, 'barri': 2, 'titanium': 2, 'screw': 2, 'sure': 2, 'whi': 2, 'mon': 1, 'peter': 1, 'peltonen': 1, 'fri': 1, 'harri': 1, 'haataja': 1, 'local': 1, 'upgrad': 1, 'from': 1, 'somewher': 1, 'plu': 1, 'orkplac': 1, 'olen': 1, 'ajatellut': 1, 'pystyttää': 1, 'itselleni': 1, 'lokaalin': 1, 'varaston': 1, 'kun': 1, 'suomesta': 1, 'ei': 1, 'tunnu': 1, 'löytyvän': 1, 'julkista': 1, 'peiliä': 1, 'osaisitko': 1, 'avittaa': 1, 'hiukan': 1, 'asiassa': 1, 'eli': 1, 'kuinka': 1, 'lähteä': 1, 'liikkeel': 1, 'ensin': 1, 'kannattane': 1, 'peilata': 1, 'varsinainen': 1, 'n': 1, 'jostain': 1, 'vaan': 1, 'millä': 1, 'softalla': 1, 'rsync': 1, 'ja': 1, 'mistä': 1, 'tuo': 1, 'kannattaa': 1, 'tehdä': 1, 'ajatuksia': 1, 'll': 1, 'post': 1, 'stori': 1, 'here': 1, 'anyway': 1, 'hope': 1, 'no': 1, 'mind': 1, 'may': 1, 'freeli': 1, 'comment': 1, 'or': 1, 'in': 1, 'anoth': 1, 'text': 1, 'tree': 1, 'd': 1, 'number_numb': 1, 'link': 1, 'instal': 1, 'imag': 1, 'throw': 1, 'with': 1, 'makefil': 1, 'after': 1, 'each': 1, 'new': 1, 'packag': 1, 'nice': 1, 'genbasedir': 1, 'progress': 1, 'work': 1, 'you': 1, 'need': 1, 'make': 1, 'releas': 1, 'file': 1, 'pinch': 1, 'exmpl': 1, 'found': 1, 'under': 1, 'apach': 1, 'key': 1, 'all': 1, 'into': 1, 'conf': 1, 'away': 1, 'fi': 1, 'slow': 1, 'tuxfamili': 1, 'when': 1, 'see': 1, 'errata': 1, 'usual': 1, 'so': 1, 'rest': 1, 'shorter': 1, 'path': 1, 'host': 1, 'whole': 1, 'load': 1, 'linux': 1, 'big': 1, 'pub': 1, 'ftp': 1, 'site': 1, 'if': 1, 'there': 1, 'definit': 1, 'mayb': 1, 'they': 1, 'might': 1, 'well': 1, 'doubt': 1, 'would': 1, 'keen': 1, 'fork': 1, 'distribut': 1, 'doesn': 1, 'an': 1, 'easi': 1, 'option': 1, 'should': 1, 'just': 1, 'start': 1, 'quick': 1, 'point': 1, 'out': 1, 'torqu': 1, 'oppos': 1, 'phillip': 1, 'we': 1, 're': 1, 'not': 1, 'matter': 1, 'even': 1, 'littl': 1, 'bit': 1, 'interest': 1, 'mac': 1, 'geek': 1, 'scare': 1, 'us': 1, 'zdnet': 1, 'powerbook': 1, 'review': 1, '_______________________________________________': 1, 'mail': 1, 'freshrpm': 1, 'net': 1}),\n",
       "       Counter({'your': 13, 'you': 12, 'and': 11, 'internet': 6, 'or': 5, 'of': 5, 'the': 5, 'to': 5, 'be': 4, 'pc': 4, 'click': 4, 'is': 3, 'have': 3, 'privaci': 3, 'protect': 3, 'will': 3, 'url': 3, 'it': 3, 'a': 3, 'delet': 3, 'els': 3, 'could': 3, 'easili': 3, 'comput': 3, 'boss': 2, 'out': 2, 'download': 2, 'ez': 2, 'softwar': 2, 'in': 2, 'cach': 2, 'histori': 2, 'not': 2, 'ani': 2, 'web': 2, 'page': 2, 'pictur': 2, 'movi': 2, 'video': 2, 'sound': 2, 'e': 2, 'mail': 2, 'log': 2, 'everyth': 2, 'do': 2, 'recov': 2, 'haunt': 2, 'how': 2, 'would': 2, 'feel': 2, 'if': 2, 'snoop': 2, 'made': 2, 'thi': 2, 'inform': 2, 'public': 2, 'children': 2, 'all': 2, 'on': 2, 'number': 2, 'here': 2, 'dear': 1, 'closr': 1, 'usag': 1, 'track': 1, 'no': 1, 'wife': 1, 'kid': 1, 'find': 1, 're': 1, 'seriou': 1, 'troubl': 1, 's': 1, 'proven': 1, 'fact': 1, 'becaus': 1, 'chat': 1, 'see': 1, 'forev': 1, 'spous': 1, 'mother': 1, 'father': 1, 'neighbor': 1, 'media': 1, 'ruin': 1, 'life': 1, 'solv': 1, 'problem': 1, 'enjoy': 1, 'benefit': 1, 'an': 1, 'as': 1, 'new': 1, 'can': 1, 'speed': 1, 'up': 1, 'browser': 1, 'reclaim': 1, 'hard': 1, 'disk': 1, 'space': 1, 'profession': 1, 'clean': 1, 'one': 1, 'easi': 1, 'mous': 1, 'did': 1, 'know': 1, 'for': 1, 'exampl': 1, 'that': 1, 'everi': 1, 'make': 1, 'window': 1, 'start': 1, 'menu': 1, 'store': 1, 'perman': 1, 'hidden': 1, 'encrypt': 1, 'databas': 1, 'within': 1, 'own': 1, 'keep': 1, 'frighten': 1, 'record': 1, 'both': 1, 'onlin': 1, 'off': 1, 'line': 1, 'activ': 1, 'anyon': 1, 'ever': 1, 'view': 1, 'even': 1, 'mani': 1, 'year': 1, 'later': 1, 'somebodi': 1, 'their': 1, 'friend': 1, 'use': 1, 'what': 1, 'they': 1, 'tri': 1, 'act': 1, 'now': 1, 'stop': 1, 'these': 1, 'file': 1, 'come': 1, 'back': 1, 'from': 1, 'dead': 1, 'remov': 1, 'fryte': 1, 'ponumb': 1, 'kj': 1, '_number': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274b143",
   "metadata": {},
   "source": [
    "## Convert Counters to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f256a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "    # Aprende quais palavras aparecem mais nos emails\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10) # Limita a contagem máxima a 10 para evitar que palavras muito frequentes dominem a contagem\n",
    "        # Pega apenas as palavras mais comuns\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        # Cria dicionario que associa palavras a indices numericos\n",
    "        # começando de 1\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Criaçao da matriz esparsa\n",
    "    # Colunas : Email, 0, 1, 2, ... n (indices das palavras, sendo 0  para desconhecidas)\n",
    "    # cada linha contem o numero do email e a frequencia de cada palavra\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48cd5dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int32'\n",
       "\twith 30 stored elements and shape (3, 11)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba05884b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165,   6,   9,   4,   5,   3,   2,   1,   3,   2,   0],\n",
       "       [325,   6,  11,  11,   7,   0,   1,   8,   3,   6,  10],\n",
       "       [227,  11,   2,   5,   5,  13,  12,   3,   5,   2,   0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60183c",
   "metadata": {},
   "source": [
    "Vocabulário limitado, resolver isso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dc13d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1,\n",
       " 'number': 2,\n",
       " 'to': 3,\n",
       " 'the': 4,\n",
       " 'your': 5,\n",
       " 'you': 6,\n",
       " 'a': 7,\n",
       " 'of': 8,\n",
       " 'on': 9,\n",
       " 'i': 10}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f16d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b00e7f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.950) total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.970) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.965) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9619929734908975"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0d955ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 94.34%\n",
      "Recall: 93.28%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1690d6f6",
   "metadata": {},
   "source": [
    "# Testing other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b76ad6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao treinar LogisticRegressionCV: If using all scalar values, you must pass an index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Accuracy, Balanced Accuracy, Precision, Recall, F1-Score]\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import ensemble, linear_model, naive_bayes, neighbors, svm, tree\n",
    "import pandas as pd\n",
    "\n",
    "clf_models = [\n",
    "    #Ensemble Methods\n",
    "#     ensemble.AdaBoostClassifier(),\n",
    "#     ensemble.BaggingClassifier(),\n",
    "#     ensemble.ExtraTreesClassifier(),\n",
    "#     ensemble.GradientBoostingClassifier(),\n",
    "#     ensemble.RandomForestClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(max_iter=1000, random_state=42),\n",
    "#     linear_model.RidgeClassifierCV(),\n",
    "#     linear_model.Perceptron(),\n",
    "    \n",
    "#     #Navies Bayes\n",
    "#     naive_bayes.BernoulliNB(),\n",
    "#     naive_bayes.GaussianNB(),\n",
    "    \n",
    "#     #Nearest Neighbor\n",
    "#     neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "#     #SVM\n",
    "#     svm.SVC(probability=True),\n",
    "    \n",
    "#     #Trees    \n",
    "#     tree.DecisionTreeClassifier() \n",
    "    ]\n",
    "\n",
    "columns = ['Name', 'Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "df_metrics = pd.DataFrame(columns = columns)\n",
    "\n",
    "for clf in clf_models:\n",
    "    try:\n",
    "        clf.fit(X_train_transformed, y_train)\n",
    "        y_pred = clf.predict(X_test_transformed)\n",
    "        #y_prob = clf.predict_proba(X_test_transformed)[:, 1] if hasattr(clf, \"predict_proba\") else None\n",
    "        \n",
    "        \n",
    "        metrics = {\n",
    "            'Name': clf.__class__.__name__,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1-Score': f1_score(y_test, y_pred),\n",
    "            #'AUC': roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "        }\n",
    "        \n",
    "        df_metrics = pd.concat([df_metrics, pd.DataFrame(metrics, index=['Name'])])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao treinar {clf.__class__.__name__}: {e}\")\n",
    "\n",
    "# Exibindo os resultados\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b3714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5c8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e74307c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
